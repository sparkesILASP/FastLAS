# Script for testing bounded FastLAS.

# Roughly:

# There are 5 distinct datasets and as possibilities get large testing may be done on any subset of these.
# For the datasets considered, a percentage of examples are set aside for training and the rest are used for testing.

# For training, the examples are concatenated and passed to FastLAS.
# For testing, the context of each example is extracted, shared background added, along with an evaluation program and this is all sent to clingo.
# The testing program keeps track of true positive (correct splits), false negatives (incorrect absence of split), etc. and this is used to provide an F1 score.

# A max_bound variable is used to set the bound across all examples for training, if desired.
# If set to -1 the variable is ignored and the bound present in the examples is used.
# The bound present in the examples is equal to the total number of true splits for the example.

# TODO:

# For comparison, the same possibilities generated by FastLAS for bounded examples are reused as opl tasks with the bound set to a penalty.
# The exact penalty is given by the noise_penalty variable.
# These are concatenated and passed to clingo.
# Testing is the same as for bounds.

# Output gives the training examples used per dataset, the hypothesis learnt, and F1 data.


import os
import re
import math
import random
from pathlib import Path
import subprocess

missing_rules = r'''
prevprevpos(P,X) :- pos(P,(X + 2)).
'''

bias = r'''
#modeh(split(var(token))).
#maxv(1).
#modeb(1, pos(const(postype),var(token))).
#modeb(1, prevpos(const(postype),var(token))).
#bias("penalty(2, head(X)) :- in_head(X).").
#bias("penalty(1, body(X)) :- in_body(X).").
'''

evaluation_program = r'''
tp(N) :- true_split(N), split(N).
tn(N) :- not true_split(N), not split(N), possible_split(N).
fp(N) :- not true_split(N), split(N).
fn(N) :- true_split(N), not split(N).

tp_sum(M) :- M = #sum{ 1 : tp(N) }.
tn_sum(M) :- M = #sum{ 1 : tn(N) }.
fp_sum(M) :- M = #sum{ 1 : fp(N) }.
fn_sum(M) :- M = #sum{ 1 : fn(N) }.

#show tp_sum/1.
#show tn_sum/1.
#show fp_sum/1.
#show fn_sum/1.
'''

files_dir = "../data/sentence_chunking/files"
collection_dir = files_dir + "/separate_bes"

training_size_percent = 0.005

max_bound = 3
noise_penalty = 2

for_consideration = set([
  "studenta_sent12_be",
  # "headlines_sent1_be",
  # "headlines_sent2_be",
  # "images_sent1_be",
  # "images_sent2_be",
])

bound_lines = []
collection_description = ""
unified_background = ""

with os.scandir(collection_dir) as collections:
  
  for collection in collections:
    if collection.name in for_consideration:

      collection_description += collection.name
      bound_lines.append("% % " + collection.name + "\n\n")

      with os.scandir(collection_dir + "/" + collection.name) as examples:
        example_count = 0
        for example in examples:
          example_count += 1
        
        training_amount = math.floor(example_count * training_size_percent)

        training_example_numbers = set(random.sample(range(1,example_count),training_amount))

        print(collection.name + " training examples: " + str(training_example_numbers))

      with os.scandir(collection_dir + "/" + collection.name) as examples:
        
        for example in examples:
          # combine all the examples to a file to train, substituting max bound if set
          if example.name[:2] == "id" and int(example.name[3:-3]) in training_example_numbers:
            line_count = 0
            with open(collection_dir + "/" + collection.name + "/" + example.name, "r") as file:
              lines = file.readlines()
              for line in lines:
                line_count += 1
                # ensure ids are unique
                if line[:3] == "#be":
                  bound_lines.append("#be(" + collection.name + line[6:])
                # the third line is the bound so replace with max if set
                elif line_count == 3 and max_bound != -1:
                  bound_lines.append("\t " + str(max_bound) + ",\n")
                else:
                  bound_lines.append(line)
              bound_lines.append("\n\n")
              file.close()
    
      with open(collection_dir + "/" + collection.name + "/background.lp") as background:
        lines = background.read()
        for line in lines:
          bound_lines.append(line)
          unified_background += line
        bound_lines.append("\n\n")
        background.close()


  with open(files_dir + "/" + collection_description + "_train.las", "w") as file:
    for line in bound_lines:
      file.write(line)
    file.write(unified_background)
    file.write(missing_rules)
    file.write(bias)
    file.close()
    
  # bounded
  print("Bounded\n\n")
  result = subprocess.run(["./FastLAS", "--bound", os.path.join(files_dir, collection_description + "_train.las")], capture_output=True)
  solution = re.search(r'# Solution:([^#]*)', result.stdout.decode("utf-8"))[1]
  
  print("Hypothesis: " + solution)

  fn_count_b = 0
  fp_count_b = 0
  tn_count_b = 0
  tp_count_b = 0
  

  with os.scandir(collection_dir) as collections:
    for collection in collections:
     if collection.name in for_consideration:
      with os.scandir(collection_dir + "/" + collection.name) as examples:
        for example in examples:
          if example.name[:2] == "id" and int(example.name[3:-3]) not in training_example_numbers:
            with open(collection_dir + "/" + collection.name + "/" + example.name, "r") as file:
              context = re.match(r'[^{]*{[^{]*{[^{]*{([^}]*)', str(file.read()))
              if context:
                example_eval_program = context[1] + unified_background + solution + evaluation_program
                example_eval_program = example_eval_program
                with open('temp.las', 'w') as file:
                  file.write(example_eval_program)
                  file.close()

                result = subprocess.run(["Clingo", 'temp.las'], capture_output=True)
                result_out = str(result.stdout)

                fn_search = re.search(r'fn_sum\((\d+)\)', result_out)
                if fn_search:
                  fn_count_b += int(fn_search[1])
                fp_search = re.search(r'fp_sum\((\d+)\)', result_out)
                if fp_search:
                  fp_count_b += int(fp_search[1])
                tn_search = re.search(r'tn_sum\((\d+)\)', result_out)
                if tn_search:
                  tn_count_b += int(tn_search[1])
                tp_search = re.search(r'tp_sum\((\d+)\)', result_out)
                if tp_search:
                  tp_count_b += int(tp_search[1])
              file.close()

  print("fn: " + str(fn_count_b))
  print("fp: " + str(fp_count_b))
  print("tn: " + str(tn_count_b))
  print("tp: " + str(tp_count_b))
  f1_b = tp_count_b / (tp_count_b + .5 * (fp_count_b + fn_count_b))
  print("f1: " + str(f1_b))


  # noisy
  print("Noisy\n\n")
  noise_examples = subprocess.run(["./FastLAS", "--bound", "--show-p", os.path.join(files_dir, collection_description + "_train.las")], capture_output=True).stdout.decode("utf-8")

  with open(files_dir + "/" + collection_description + "_train_noise.las", "w") as noisy_file:
    # noisy_file.write(noise_examples)
    noisy_file.write(re.sub(r'@\d+', "@" + str(noise_penalty), noise_examples))
    noisy_file.write(unified_background)
    noisy_file.write(missing_rules)
    noisy_file.write(bias)
    noisy_file.close()

  noisy_result = subprocess.run(["./FastLAS", "--opl", os.path.join(files_dir, collection_description + "_train_noise.las")], capture_output=True)
  solution = noisy_result.stdout.decode("utf-8")

  print("Hypothesis: " + solution)

  fn_count_n = 0
  fp_count_n = 0
  tn_count_n = 0
  tp_count_n = 0
  
  with os.scandir(collection_dir) as collections:
    for collection in collections:
     if collection.name in for_consideration:
      with os.scandir(collection_dir + "/" + collection.name) as examples:
        for example in examples:
          if example.name[:2] == "id" and int(example.name[3:-3]) not in training_example_numbers:
            with open(collection_dir + "/" + collection.name + "/" + example.name, "r") as file:
              context = re.match(r'[^{]*{[^{]*{[^{]*{([^}]*)', str(file.read()))
              if context:
                example_eval_program = context[1] + unified_background + solution + evaluation_program
                example_eval_program = example_eval_program
                with open('temp.las', 'w') as file:
                  file.write(example_eval_program)
                  file.close()

                result = subprocess.run(["Clingo", 'temp.las'], capture_output=True)
                result_out = str(result.stdout)

                fn_search = re.search(r'fn_sum\((\d+)\)', result_out)
                if fn_search:
                  fn_count_n += int(fn_search[1])
                fp_search = re.search(r'fp_sum\((\d+)\)', result_out)
                if fp_search:
                  fp_count_n += int(fp_search[1])
                tn_search = re.search(r'tn_sum\((\d+)\)', result_out)
                if tn_search:
                  tn_count_n += int(tn_search[1])
                tp_search = re.search(r'tp_sum\((\d+)\)', result_out)
                if tp_search:
                  tp_count_n += int(tp_search[1])
              file.close()

  print("fn: " + str(fn_count_n))
  print("fp: " + str(fp_count_n))
  print("tn: " + str(tn_count_n))
  print("tp: " + str(tp_count_n))
  f1_n = tp_count_n / (tp_count_n + .5 * (fp_count_n + fn_count_n))
  print("f1: " + str(f1_n))

  print("\n\nBounded F1: \t" + str(f1_b))
  print("Noisy F1: \t" + str(f1_n))

  

  


  # noisy_p_result = subprocess.run(["./FastLAS", "--show-p", "--bound", os.path.join(files_dir, collection_description + "_train.las")], capture_output=True)
  # print(noisy_p_result.stdout)


