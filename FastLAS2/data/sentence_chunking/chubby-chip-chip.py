# Script for testing bounded FastLAS.

# Roughly:

# There are 5 distinct datasets and as possibilities get large testing may be done on any subset of these.
# For the datasets considered, a percentage of examples are set aside for training and the rest are used for testing.

# For training, the examples are concatenated and passed to FastLAS.
# For testing, the context of each example is extracted, shared background added, along with an evaluation program and this is all sent to clingo.
# The testing program keeps track of true positive (correct splits), false negatives (incorrect absence of split), etc. and this is used to provide an F1 score.

# A max_bound variable is used to set the bound across all examples for training, if desired.
# If set to -1 the variable is ignored and the bound present in the examples is used.
# The bound present in the examples is equal to the total number of true splits for the example.

# TODO:

# For comparison, the same possibilities generated by FastLAS for bounded examples are reused as opl tasks with the bound set to a penalty.
# The exact penalty is given by the example_penalty variable.
# These are concatenated and passed to clingo.
# Testing is the same as for bounds.

# Output gives the training examples used per dataset, the hypothesis learnt, and F1 data.


import os
import re
import math
import random
from pathlib import Path
import subprocess

missing_rules = r'''
prevprevpos(P,X) :- pos(P,(X + 2)).
'''

bias = r'''
#modeh(split(var(token))).
#maxv(1).
#modeb(1, pos(const(postype),var(token))).
#modeb(1, prevpos(const(postype),var(token))).
#bias("penalty(2, head(X)) :- in_head(X).").
#bias("penalty(1, body(X)) :- in_body(X).").
'''

evaluation_program = r'''
tp(N) :- true_split(N), split(N).
tn(N) :- not true_split(N), not split(N), possible_split(N).
fp(N) :- not true_split(N), split(N).
fn(N) :- true_split(N), not split(N).

tp_sum(M) :- M = #sum{ 1 : tp(N) }.
tn_sum(M) :- M = #sum{ 1 : tn(N) }.
fp_sum(M) :- M = #sum{ 1 : fp(N) }.
fn_sum(M) :- M = #sum{ 1 : fn(N) }.

#show tp_sum/1.
#show tn_sum/1.
#show fp_sum/1.
#show fn_sum/1.
'''

files_dir = "../data/sentence_chunking/files"
collection_dir = files_dir + "/separate_bes"

training_size_percent = 0.05

max_bound = 3
example_penalty = 2

for_consideration = set([
  "studenta_sent12_be",
  # "headlines_sent1_be",
  # "headlines_sent2_be",
  # "images_sent1_be",
  # "images_sent2_be",
])

bound_lines = []
collection_description = ""
unified_background = ""

with os.scandir(collection_dir) as collections:
  for collection in collections:
    if collection.name in for_consideration:

      collection_description += collection.name
      bound_lines.append("% % " + collection.name + "\n\n")

      with os.scandir(collection_dir + "/" + collection.name) as examples:
        example_count = 0
        for example in examples:
          example_count += 1
        
        training_amount = math.floor(example_count * training_size_percent)

        training_example_numbers = set(random.sample(range(1,example_count),training_amount))

        print(collection.name + " training examples: " + training_example_numbers)

      with os.scandir(collection_dir + "/" + collection.name) as examples:
        
        for example in examples:
          # combine all the examples to a file to train, substituting max bound if set
          if example.name[:2] == "id" and int(example.name[3:-3]) in training_example_numbers:
            line_count = 0
            with open(collection_dir + "/" + collection.name + "/" + example.name, "r") as file:
              lines = file.readlines()
              for line in lines:
                line_count += 1
                # ensure ids are unique
                if line[:3] == "#be":
                  bound_lines.append("#be(" + collection.name + line[6:])
                # the third line is the bound so replace with max if set
                elif line_count == 3 and max_bound != -1:
                  bound_lines.append("\t " + str(max_bound) + ",\n")
                else:
                  bound_lines.append(line)
              bound_lines.append("\n\n")
    
      with open(collection_dir + "/" + collection.name + "/background.lp") as background:
        lines = background.read()
        for line in lines:
          bound_lines.append(line)
          unified_background += line
        bound_lines.append("\n\n")

  # if not os.path.exists(collection_dir + "/train_1"):
  #   os.makedirs(collection_dir + "/train_1")

  with open(files_dir + "/" + collection_description + "_train.las", "w") as file:
    for line in bound_lines:
      file.write(line)
    file.write(missing_rules)
    file.write(bias)
    file.close()
    
  # bounded
  result = subprocess.run(["./FastLAS", "--bound", os.path.join(files_dir, collection_description + "_train.las")], capture_output=True)
  solution = re.search(r'# Solution:([^#]*)', str(result.stdout))[1].replace('\\n', ' ')
  
  print("Hypothesis: " + solution)

  fn_count = 0
  fp_count = 0
  tn_count = 0
  tp_count = 0
  

  with os.scandir(collection_dir) as collections:
    for collection in collections:
     if collection.name in for_consideration:
      with os.scandir(collection_dir + "/" + collection.name) as examples:
        for example in examples:
          if example.name[:2] == "id" and int(example.name[3:-3]) not in training_example_numbers:
            with open(collection_dir + "/" + collection.name + "/" + example.name, "r") as file:
              context = re.match(r'[^{]*{[^{]*{[^{]*{([^}]*)', str(file.read()))
              if context:
                example_eval_program = context[1] + unified_background + solution + evaluation_program
                example_eval_program = example_eval_program
                with open('temp.las', 'w') as file:
                  file.write(example_eval_program)
                  file.close()

                result = subprocess.run(["Clingo", 'temp.las'], capture_output=True)
                result_out = str(result.stdout)

                fn_search = re.search(r'fn_sum\((\d+)\)', result_out)
                if fn_search:
                  fn_count += int(fn_search[1])
                fp_search = re.search(r'fp_sum\((\d+)\)', result_out)
                if fp_search:
                  fp_count += int(fp_search[1])
                tn_search = re.search(r'tn_sum\((\d+)\)', result_out)
                if tn_search:
                  tn_count += int(tn_search[1])
                tp_search = re.search(r'tp_sum\((\d+)\)', result_out)
                if tp_search:
                  tp_count += int(tp_search[1])

  print("fn: " + str(fn_count))
  print("fp: " + str(fp_count))
  print("tn: " + str(tn_count))
  print("tp: " + str(tp_count))
  print("f1: " + str(tp_count / (tp_count + .5 * (fp_count + fn_count))))





  # noisy
  # noisy_p_result = subprocess.run(["./FastLAS", "--show-p", "--bound", os.path.join(files_dir, collection_description + "_train.las")], capture_output=True)
  # print(noisy_p_result.stdout)


